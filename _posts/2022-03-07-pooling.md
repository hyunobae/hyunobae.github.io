---
title: Pooling에 대한 고찰
author: hyunobae
categories: [Pytorch]
tags: [Pytorch]
published: true
---
# Pooling
이번에는 딥 러닝 모델에서 자주 사용되는 pooling에 대해서 정리하도록 하겠다. Pooling은 컴퓨터비전, 특히 CNN에서 많이 사용되는데 고차원의 feature를 축소하는 용도로 많이 사용이 되고 있다. 크기는 줄이면서 중요한 정보는 강조하는 중요한 역할을 맡고 있다. Pooling에는 여러가지 종류가 있는데 아래에서 하나씩 보도록 하겠다.

## Max pooling & Average pooling
<center><img src=https://user-images.githubusercontent.com/54826050/156974070-099b23c2-bbd3-4248-a624-828f875963fe.png></center><br>

Max pooling은 말 그대로 **최대로 pooling을 수행**하겠다고 이해하면 쉽다. 아래 그림은 maxpooling2D의 예시이다. 2x2 maxpooling은  2x2 크기의 영역에서 max pooling을 수행한다는 의미이다. 본 예시는 stride가 2인 경우이다. *Stride는 보폭인데, 2x2 크기의 사각형이 다음 max pooling을 수행할 때 옆으로 얼마나 움직일 것인지 정하는 파라미터이다.* Max pooling을 수행하면 각 2x2 영역에서 가장 큰 수가 선택되는 것을 확인할 수 있다. 예를 들어, 빨간 영역의 [12, 20, 8, 12]에서 **max pooling을 수행하면 가장 큰 수인 20이 선택되는 것**을 의미한다. <br>
Average pooling의 경우, max pooling과 다르게 특정 영역의 가장 큰 값이 아닌, **평균 값을 계산하여 사용**한다. 예를 들어, [12, 20, 8, 12]에서 average pooling을 수행하면 평균값인 13이 계산되는 것이다. 이외의 부분들은 max pooling과 동일하다.<br>
이러한 pooling을 사용하는 이유는 다음과 같다. - CNN에서 다양한 feature를 추출하여 학습하기 위해 많은 수의 filter를 사용하게 된다. 
- 많은 filter를 사용하면 많은 feature map이 생성되어 파라미터의 수가 굉장히 커진다.
- 이때, pooling 기법을 사용하여 feature map의 크기를 줄여서 파라미터 수를 줄여서 학습에 도움을 준다.

## Global Average pooling

<img width="696" alt="global_average_pooling" src="https://user-images.githubusercontent.com/54826050/156975402-9cd230ad-172f-40c9-af98-a5b85effc69c.png"><br>

Global Average pooling (GAP) 는 앞선 pooling들과는 사용하는 원인이 다르다. 그 전에 documentation을 먼저 보자. Pytorch에서는 [AdaptiveAvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)라 불리는데 여타 다른 pooling과 비슷하게 input과 output의 channel 갯수는 유지가 되는 것을 확인할 수 있다. 그렇다면 GAP는 언제 사용되는가? <br>
실제 용례를 찾아보면 다른 pooling이 convolution층 이후에 사용되는 것과 다르게 GAP는 dense layer를 없애고, 이를 convolution으로 대체할 때 사용된다. 다시 말해, ***dense layer를 사용하지 않기 위함이라는 말이다.*** GAP를 사용하면 다음과 같은 일이 벌어진다.
- 기존 Fully-connected layer를 사용하지 않을 수 있기 떄문에 계산량에서 엄청난 이득을 얻음
- 각 channel이 가지는 값을 **단일 평균 값**으로 대체하기 때문에 *정보의 손실이 굉장히 큼* -> (batch, H, W, C)가 (batch, 1, 1, C)로 변환
- 그럼에도 성능이 좋음<br>

*어떻게 feature map을 단일 평균값으로 치환하는데 성능이 좋을 수 있는지 궁금했다. 정확한 답은 안되지만 이를 설명하는데 도움이 될만한 논문이 있었다.*

[Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150) 논문에서는 ResNet에서 GAP layer 직전의 activation map은 이미지에서 서로 다른 패턴을 발견하는 detector처럼 작용한다고 한다. 
실험을 위해 아래와 같이 모델을 구성하였다.
<center><img src= https://user-images.githubusercontent.com/54826050/156990752-6354228b-adab-4912-8219-0c69f9bb06de.jpg></center>

activation에서 GAP를 거치며 feature map의 평균을 가져서 (7, 7, 2048)이 (1, 1, 2048)이 되는 것을 확인 할 수 있다. Flatten()을 통해 모든 GAP node가 dense layer의 모든 node와 연결된다. <br>

<center><img src=https://user-images.githubusercontent.com/54826050/157002610-cfc268c6-2384-4887-b037-d2bde3820d1c.PNG></center>

위 그림은 class activation map을 구하는 과정을 간략하게 설명한 그림이다. 각 feature map의 값을 통해 얻은 GAP를 dense layer의 weight와 곱해 **class activation map**을 구한다. 핵심은 ***이미지의 어떤 좌표가 본 이미지의 class를 결정하는데 미치는 중요도를 heat map으로 추적해 볼 수 있다는 것이다.*** 자세한 내용은 위 논문을 참고하면 된다. 예측컨데, GAP는 결국 feature map의 전체 값(합)을 모두 이용하기 때문에 feature map의 특성을 어느정도 유지하여 반영해주는 것 같다. 이 부분은 더 알아보면 재밌는 부분이 될 것 같다.


#### Image & source
- https://computersciencewiki.org/index.php/File:MaxpoolSample2.png
- https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/
- https://kangbk0120.github.io/articles/2018-02/cam
