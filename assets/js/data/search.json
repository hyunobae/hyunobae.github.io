[ { "title": "소수만들기 (Python)", "url": "/posts/make_primenum/", "categories": "algorithm", "tags": "algorithm, 프로그래머스, combinations", "date": "2022-05-17 00:00:00 +0800", "snippet": "문제 설명주어진 숫자 중 3개의 수를 더했을 때 소수가 되는 경우의 개수를 구하려고 합니다. 숫자들이 들어있는 배열 nums가 매개변수로 주어질 때, nums에 있는 숫자들 중 서로 다른 3개를 골라 더했을 때 소수가 되는 경우의 개수를 return 하도록 solution 함수를 완성해주세요.제한 사항 nums에 들어있는 숫자의 개수는 3개 이상 50개 이하입니다. nums의 각 원소는 1 이상 1,000 이하의 자연수이며, 중복된 숫자가 들어있지 않습니다.풀이itertools의 combinations를 알고 있는가가 중요한 문제인 것 같다. combinations로 가능한 조합을 만들고 더하고 소수인지 확인하는 loop면 간단하게 해결 가능하다.코드from itertools import combinationsdef solution(nums): comb = list(combinations(nums, 3)) answer = 0 for i in range(len(comb)): sum = 0 cnt=0 sum = comb[i][0]+comb[i][1]+comb[i][2] for j in range(2, sum): if sum % j == 0: cnt += 1 break if cnt==0: answer+= 1 return answer" }, { "title": "프린터 (Python)", "url": "/posts/printer/", "categories": "algorithm", "tags": "algorithm, 프로그래머스, deque", "date": "2022-05-10 00:00:00 +0800", "snippet": "문제 설명일반적인 프린터는 인쇄 요청이 들어온 순서대로 인쇄합니다. 그렇기 때문에 중요한 문서가 나중에 인쇄될 수 있습니다. 이런 문제를 보완하기 위해 중요도가 높은 문서를 먼저 인쇄하는 프린터를 개발했습니다. 이 새롭게 개발한 프린터는 아래와 같은 방식으로 인쇄 작업을 수행합니다. 인쇄 대기목록의 가장 앞에 있는 문서(J)를 대기목록에서 꺼냅니다. 나머지 인쇄 대기목록에서 J보다 중요도가 높은 문서가 한 개라도 존재하면 J를 대기목록의 가장 마지막에 넣습니다. 그렇지 않으면 J를 인쇄합니다. 예를 들어, 4개의 문서(A, B, C, D)가 순서대로 인쇄 대기목록에 있고 중요도가 2 1 3 2 라면 C D A B 순으로 인쇄하게 됩니다.내가 인쇄를 요청한 문서가 몇 번째로 인쇄되는지 알고 싶습니다. 위의 예에서 C는 1번째로, A는 3번째로 인쇄됩니다.현재 대기목록에 있는 문서의 중요도가 순서대로 담긴 배열 priorities와 내가 인쇄를 요청한 문서가 현재 대기목록의 어떤 위치에 있는지를 알려주는 location이 매개변수로 주어질 때, 내가 인쇄를 요청한 문서가 몇 번째로 인쇄되는지 return 하도록 solution 함수를 작성해주세요.제한 사항 현재 대기목록에는 1개 이상 100개 이하의 문서가 있습니다. 인쇄 작업의 중요도는 1~9로 표현하며 숫자가 클수록 중요하다는 뜻입니다. location은 0 이상 (현재 대기목록에 있는 작업 수 - 1) 이하의 값을 가지며 대기목록의 가장 앞에 있으면 0, 두 번째에 있으면 1로 표현합니다.입출력 예|priorities|location|return||—|—|—||[2, 1, 3, 2]|2|1||[1, 1, 9, 1, 1, 1|0|5풀이인쇄 대기목록의 가장 앞에 있는 문서를 꺼내고, 중요도에 따라 가장 뒤쪽으로 넣는 것을 보고 바로 deque를 사용하기로 했다. deque의 popleft()와 append를 사용하면 쉽게 풀 수 있겠다고 생각했는데 깔끔하게 맞아 떨어진 것 같다.코드from collections import dequedef solution(priorities, location): answer = 0 cur_idx = location queue = deque(priorities) reappend = 0 cnt = 0 while True: maxnum = max(queue) job = queue.popleft() cnt += 1 cur_idx-=1 if cur_idx==-1 and maxnum&amp;lt;=job: break if maxnum&amp;gt;job: queue.append(job) reappend+=1 if cur_idx&amp;lt;0: cur_idx = len(queue)-1 answer = cnt - reappend return answerdeque를 사용하는 구현 문제인 것 같다. 문제를 그대로 따라가면 해결할 수 있는 문제였다. cur_idx는 내가 요청한 문서를 지칭하는 index이다." }, { "title": "더 맵게 (Python)", "url": "/posts/heapq/", "categories": "algorithm", "tags": "algorithm, 프로그래머스, heap", "date": "2022-05-09 00:00:00 +0800", "snippet": "문제 설명매운 것을 좋아하는 Leo는 모든 음식의 스코빌 지수를 K 이상으로 만들고 싶습니다. 모든 음식의 스코빌 지수를 K 이상으로 만들기 위해 Leo는 스코빌 지수가 가장 낮은 두 개의 음식을 아래와 같이 특별한 방법으로 섞어 새로운 음식을 만듭니다. 섞은 음식의 스코빌 지수 = 가장 맵지 않은 음식의 스코빌 지수 + (두 번째로 맵지 않은 음식의 스코빌 지수 * 2)Leo는 모든 음식의 스코빌 지수가 K 이상이 될 때까지 반복하여 섞습니다.Leo가 가진 음식의 스코빌 지수를 담은 배열 scoville과 원하는 스코빌 지수 K가 주어질 때, 모든 음식의 스코빌 지수를 K 이상으로 만들기 위해 섞어야 하는 최소 횟수를 return 하도록 solution 함수를 작성해주세요.제한 사항 scoville의 길이는 2 이상 1,000,000 이하입니다. K는 0 이상 1,000,000,000 이하입니다. scoville의 원소는 각각 0 이상 1,000,000 이하입니다. 모든 음식의 스코빌 지수를 K 이상으로 만들 수 없는 경우에는 -1을 return 합니다.입출력 예|scoville|K|2||—|:—:|—:||[1, 2, 3, 9, 10, 12]|7|2|풀이처음에는 deque로 popleft와 appendleft로 해결할 수 있겠다 생각했는데 매번 min과 다음으로 작은 min을 찾아줘야하기 때문에 시간 복잡도 측면에서 실패했다. sort()를 사용할 수 없다는 말이었기 떄문에 이는 deque로 해결 불가능. 따라서, 항상 루트에 최소 값을 가지는 min heap 사용. min heap은 모든 원소는 자식 원소들보다 크기가 작거나 같도록 유지되는 것이 특징이다. 따라서, sort를 따로 할 필요가 없다.코드import heapqdef solution(scoville, K): answer = 0 heap = [] for i in scoville: # heapq.heapify(list) -&amp;gt; 기존 list를 heapq로 변환 heapq.heappush(heap, i) while heap[0]&amp;lt;K: try: heapq.heappush(heap, (heapq.heappop(heap)+heapq.heappop(heap)*2)) except IndexError: return -1 answer+= 1 return answer이번 문제에서 heap의 사용 방법을 새로 알게 되었는데 heapq.heapify()를 하면 기존 list를 heapq로 바꿔준다고 한다. 넘 중요.. 가장 작은 원소가 스코빌 K보다 작으면 계속 루프 실행 스코빌 공식에 따라 heap 삭제 및 추가 수행 heap 내에 더 이상 원소가 없는 경우 -&amp;gt; return -1" }, { "title": "기능개발 (Python)", "url": "/posts/deque/", "categories": "algorithm", "tags": "algorithm, 프로그래머스, deque", "date": "2022-05-09 00:00:00 +0800", "snippet": "문제 설명프로그래머스 팀에서는 기능 개선 작업을 수행 중입니다. 각 기능은 진도가 100%일 때 서비스에 반영할 수 있습니다.또, 각 기능의 개발속도는 모두 다르기 때문에 뒤에 있는 기능이 앞에 있는 기능보다 먼저 개발될 수 있고, 이때 뒤에 있는 기능은 앞에 있는 기능이 배포될 때 함께 배포됩니다.먼저 배포되어야 하는 순서대로 작업의 진도가 적힌 정수 배열 progresses와 각 작업의 개발 속도가 적힌 정수 배열 speeds가 주어질 때 각 배포마다 몇 개의 기능이 배포되는지를 return 하도록 solution 함수를 완성하세요.제한 사항 작업의 개수(progresses, speeds배열의 길이)는 100개 이하입니다. 작업 진도는 100 미만의 자연수입니다. 작업 속도는 100 이하의 자연수입니다. 배포는 하루에 한 번만 할 수 있으며, 하루의 끝에 이루어진다고 가정합니다. 예를 들어 진도율이 95%인 작업의 개발 속도가 하루에 4%라면 배포는 2일 뒤에 이루어집니다.입출력 예|progresses|speeds|return||—|—|—||[93, 30, 55]|[1, 30, 5]|[2, 1]||[95, 90, 99, 99, 80, 99]|[1, 1, 1, 1, 1, 1]|[1, 3, 2]|풀이우선 ‘배포되어야 하는 순서가 있다’를 통해 queue를 사용해서 해결할 수 있겠다는 생각을 했다. 고민 없이 deque를 사용하기로 했다.코드from collections import dequedef solution(progresses, speeds): answer = [] queue = deque(progresses) squeue = deque(speeds) cnt = 0 is_fin = 0 while queue: for i in range(len(queue)): queue[i] += squeue[i] for i in range(len(queue)): if queue[0]&amp;gt;=100 and is_fin==0: is_fin = 1 cnt += 1 if is_fin and (i+1)&amp;lt;len(queue) and queue[i+1]&amp;gt;= 100: cnt += 1 else: break for j in range(cnt): queue.popleft() squeue.popleft() if cnt != 0 : answer.append(cnt) cnt = 0 is_fin = 0 return answer다른 분들 풀이는 굉장히 깔끔하던데 본인은 생각나는대로 짜봤다. progresses와 speed를 더해주며 첫 번째 작업이 끝난지 확인한다. 첫 번째 작업이 끝났을 때만 바로 다음 이어지는 작업의 완료 유무를 확인한다. 당일 완료된 작업이 있다면 그 개수만큼 answer에 넣어준다." }, { "title": "Pooling에 대한 고찰", "url": "/posts/pooling/", "categories": "Pytorch", "tags": "Pytorch", "date": "2022-03-07 00:00:00 +0800", "snippet": "Pooling이번에는 딥 러닝 모델에서 자주 사용되는 pooling에 대해서 정리하도록 하겠다. Pooling은 컴퓨터비전, 특히 CNN에서 많이 사용되는데 고차원의 feature를 축소하는 용도로 많이 사용이 되고 있다. 크기는 줄이면서 중요한 정보는 강조하는 중요한 역할을 맡고 있다. Pooling에는 여러가지 종류가 있는데 아래에서 하나씩 보도록 하겠다.Max pooling &amp;amp; Average pooling&amp;lt;img src=https://user-images.githubusercontent.com/54826050/156974070-099b23c2-bbd3-4248-a624-828f875963fe.png&amp;gt;Max pooling은 말 그대로 최대로 pooling을 수행하겠다고 이해하면 쉽다. 아래 그림은 maxpooling2D의 예시이다. 2x2 maxpooling은 2x2 크기의 영역에서 max pooling을 수행한다는 의미이다. 본 예시는 stride가 2인 경우이다. Stride는 보폭인데, 2x2 크기의 사각형이 다음 max pooling을 수행할 때 옆으로 얼마나 움직일 것인지 정하는 파라미터이다. Max pooling을 수행하면 각 2x2 영역에서 가장 큰 수가 선택되는 것을 확인할 수 있다. 예를 들어, 빨간 영역의 [12, 20, 8, 12]에서 max pooling을 수행하면 가장 큰 수인 20이 선택되는 것을 의미한다. Average pooling의 경우, max pooling과 다르게 특정 영역의 가장 큰 값이 아닌, 평균 값을 계산하여 사용한다. 예를 들어, [12, 20, 8, 12]에서 average pooling을 수행하면 평균값인 13이 계산되는 것이다. 이외의 부분들은 max pooling과 동일하다.이러한 pooling을 사용하는 이유는 다음과 같다. - CNN에서 다양한 feature를 추출하여 학습하기 위해 많은 수의 filter를 사용하게 된다. 많은 filter를 사용하면 많은 feature map이 생성되어 파라미터의 수가 굉장히 커진다. 이때, pooling 기법을 사용하여 feature map의 크기를 줄여서 파라미터 수를 줄여서 학습에 도움을 준다.Global Average poolingGlobal Average pooling (GAP) 는 앞선 pooling들과는 사용하는 원인이 다르다. 그 전에 documentation을 먼저 보자. Pytorch에서는 AdaptiveAvgPool2d라 불리는데 여타 다른 pooling과 비슷하게 input과 output의 channel 갯수는 유지가 되는 것을 확인할 수 있다. 그렇다면 GAP는 언제 사용되는가? 실제 용례를 찾아보면 다른 pooling이 convolution층 이후에 사용되는 것과 다르게 GAP는 dense layer를 없애고, 이를 convolution으로 대체할 때 사용된다. 다시 말해, dense layer를 사용하지 않기 위함이라는 말이다. GAP를 사용하면 다음과 같은 일이 벌어진다. 기존 Fully-connected layer를 사용하지 않을 수 있기 떄문에 계산량에서 엄청난 이득을 얻음 각 channel이 가지는 값을 단일 평균 값으로 대체하기 때문에 정보의 손실이 굉장히 큼 -&amp;gt; (batch, H, W, C)가 (batch, 1, 1, C)로 변환 그럼에도 성능이 좋음어떻게 feature map을 단일 평균값으로 치환하는데 성능이 좋을 수 있는지 궁금했다. 정확한 답은 안되지만 이를 설명하는데 도움이 될만한 논문이 있었다.Learning Deep Features for Discriminative Localization 논문에서는 ResNet에서 GAP layer 직전의 activation map은 이미지에서 서로 다른 패턴을 발견하는 detector처럼 작용한다고 한다. 실험을 위해 아래와 같이 모델을 구성하였다.&amp;lt;img src= https://user-images.githubusercontent.com/54826050/156990752-6354228b-adab-4912-8219-0c69f9bb06de.jpg&amp;gt;activation에서 GAP를 거치며 feature map의 평균을 가져서 (7, 7, 2048)이 (1, 1, 2048)이 되는 것을 확인 할 수 있다. Flatten()을 통해 모든 GAP node가 dense layer의 모든 node와 연결된다. &amp;lt;img src=https://user-images.githubusercontent.com/54826050/157002610-cfc268c6-2384-4887-b037-d2bde3820d1c.PNG&amp;gt;위 그림은 class activation map을 구하는 과정을 간략하게 설명한 그림이다. 각 feature map의 값을 통해 얻은 GAP를 dense layer의 weight와 곱해 class activation map을 구한다. 핵심은 이미지의 어떤 좌표가 본 이미지의 class를 결정하는데 미치는 중요도를 heat map으로 추적해 볼 수 있다는 것이다. 자세한 내용은 위 논문을 참고하면 된다. 예측컨데, GAP는 결국 feature map의 전체 값(합)을 모두 이용하기 때문에 feature map의 특성을 어느정도 유지하여 반영해주는 것 같다. 이 부분은 더 알아보면 재밌는 부분이 될 것 같다.Image &amp;amp; source https://computersciencewiki.org/index.php/File:MaxpoolSample2.png https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/ https://kangbk0120.github.io/articles/2018-02/cam" }, { "title": "Dataloader란?", "url": "/posts/pytorch_dataloader/", "categories": "Pytorch", "tags": "Pytorch", "date": "2022-03-03 00:00:00 +0800", "snippet": "Pytorch로 딥러닝을 학습하면 빼놓을 수 없는 dataloader이다. 보통 딥러닝 시작을 keras와 sklearn의 train_test_split()으로 사용하기 때문에 dataloader를 마주하면 당황하기 마련이다. 나 또한 dataloader가 처음에는 난해했지만 오히려 사용하면서 편하다고 느끼기도 했다. 오늘은 특히 기본으로 제공되지 않는 데이터를 커스텀하여 사용할 수 있는 CustomDataloader에 대해서 작성하도록 하겠다. 지금부터 dataloader에 대해서 알아보자.Dataloader()? 우선 torch의 dataloader는 말 그대로 훈련과 검증, 테스트시 사용할 데이터를 불러와주는 모듈이다. Dataloader class는 아래와 같이 구성되어 있다.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)CustomDataloader는 보통 map-style dataset으로 구성하며, iteration마다 indices와 key를 반환해준다. 이때, 무조건 만들어줘야하는 메소드는 _getitem_()과 _len_()이다. 아래 코드를 예시로 들며 자세히 설명하도록 하겠다.CustomDataloader의 기본 틀은 다음과 같다. 아래 코드는 본인이 super-resolution 관련 연구 중 실제로 사용한 코드 중 일부이다.class CustomDataloader(Dataset): def __init__(self, dataset_dir, crop_size, upscale): super(CustomDataloader).__init__() self.dirlist = os.listdir(dataset_dir) self.image = [] for dir in self.dirlist: for imglist in dir: self.image.append(~ + imglist) self.hr_trans = transform(crop_size) self.lr_trans = trsnsform(upscale) def __getitem__(self, index): hr_img = self.hr_trans(self.image[index]) lr_img = self.lr_trans(hr_img) return hr_img, lr_img def __len__(self): return len(self.image)Dataloader ClassDataloader를 만들 땐 Dataset을 상속받아야하는데 from torch.utils.data.dataset으로부터 상속받는다._init_()init은 이름 그대로 초기화 작업을 수행한다. 작업에서 필요한 경로 설정이나 getitem에서 사용할 변수 초기화 등을 주로 한다. _getitem_()getitem은 dataloader의 객체에서 인덱스로 접근할 때 자동으로 호출된다. 따라서, 이 부분은 데이터 전처리과정을 거쳐서 return으로 훈련이나 추론에 사용될 데이터를 반환하는 부분이다._len_()사용되는 데이터의 갯수를 반환하는 부분이다. 아주 간단한 예를 들자면, train 과정에서 사용되는 데이터의 갯수가 15000장이라면 return 15000 을 작성해주는 부분이다.Torch를 사용하면서 굉장히 난해한 부분 중 하나였는데 알고보면 생각보다 어렵지 않은 부분이었다. 오늘도 프레임워크 개발자들에게 감사함을 표하며 마무리…" }, { "title": "zero_grad(), step(), 그리고 backward()에 대한 고찰", "url": "/posts/pytorch_train/", "categories": "Pytorch", "tags": "Pytorch", "date": "2022-03-02 00:00:00 +0800", "snippet": "Pytorch로 연구 관련 코드를 사용하고 있지만 그냥 당연시하는 코드들이 있다. 바로 zero_grad() 와 step(), 그리고 backward() 이다. 오늘은 각 메서드가 어떤 일을 하는지 정리하여 앞으로 헷갈리는 일이 없도록 하는 것이 좋겠다.Pytorch에서 gradient update 과정앞서 제시한 메소드를 알기 전에 먼저 pytorch에서 gradient update가 일어나는 과정을 이해할 필요가 있다.보통 torch 기반 딥러닝 모델은 아래와 같은 과정으로 gradient update가 일어난다.optimizer.zero_grad()loss.backward()optimizer.step() 여기서 optimizer는 gradient를 update하는 방법을 의미한다. 예를 들면, 어떤 작업을 수행할 때, 작업에 따라 가장 효율적인 방법이 다를 수 있다. 이때 어떤 작업에 사용 할 효율적인 방법에 해당하는 것이 optimizer이다. 그렇다면 딥러닝에서 optimizer는 어떤 역할을 하는가? 처음 제시한 대로 정답과 예측 값 사이의 loss를 줄이는 과정에서 사용하는 최적화 방법이다. (optimizer 말 그대로 최적화해주는 것 이라고 이해해도 괜찮을 것 같다.)ex) Adam, AdaGrad, RMSProp 등이제 하나하나 정리해보도록 하자.backward()backward()는 gradient update에 있어서 가장 중요한 부분이라 할 수 있다. 이름에서 유추할 수 있듯이, 우리가 흔히 아는 굉장히 복잡한 back-propagation 과정이 이 backward()를 호출함으로써 계산된다. 이는 loss와의 손실을 따지기 때문에 위 예제코드와 같이 loss.backward()를 사용하면 된다.zero_grad()Pytorch documentation에서는 아래와 같이 정리해주었다. Optimizer.zero_grad(set_to_none=False) 이는 gradient를 가지는 optimizer의 torch.Tensor를 0으로 설정하는 것이다. 이 작업이 굉장히 중요하다. 기껏 해당 iteration에서 loss를 잘 구했는데 이 전 iteration의 batch의 loss와 결과가 섞여버린다면 결국 모델의 학습은 제대로 되지 않을 것이다. zero_grad()는 optimizer가 이번 단계의 gradient만 추적할 수 있도록 이 전 단계의 gradient를 0으로 만드는 과정이다. 따라서, optimizer.zero_grad()를 호출하면 되겠다. step()마지막으로 step()은 상대적으로 간단하다. 위 과정에서 계산된 gradient를 갱신에 사용하겠다는 의미이다. gradient 갱신의 주체는 optimizer이기 때문에 optimizer.step()을 사용하면 되겠다.이렇게 pytorch를 사용하여 훈련하는 경우 항상 사용되는 부분을 정리해보았다. 다음에는 항상 헷갈리는 dataloader에 대해 정리해봐야겠다." }, { "title": "DUF - Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation", "url": "/posts/DUF/", "categories": "Paper", "tags": "Paper, VSR", "date": "2021-10-11 00:00:00 +0800", "snippet": "개요본 논문은 DUF (Dynamic Upsampling Filters)라고 많이 cite되는 것 같다. 저자는 연세대이고, CVPR 2018에서 발표되었다. 기존 방법과는 다르게 adaptive한 filter를 사용한 것이 굉장히 신선했다. 중요하다고 생각되는 부분만 작성할 예정이라 모든 부분을 다루진 않는다 (아카이빙 목적이라 고도로 압축 ver..). 틀린 부분은 알려주시면 감사하겠습니다.Abstract Video super-resolution (이하 VSR)은 deep learning 기반의 방법을 도입한 후, 많은 방법들이 제안되었지만 많은 방법들은 motion estimation과 compensation의 정확도에 굉장히 의존하였다. 저자들은 dynamic upsampling filters와 residual image를 생성하는 DNN (deep neural net)을 제안하는데, 이는 explicit motion compenmsation (ex. optical flow)을 피하기 위해 각 pixel의 local spatio-temporal neiborhood에 따라 계산된다. 또한, 새로운 data augmentation 기법을 제안한다.Introduction 전통적인 VSR 알고리즘은 여러개의 LR frame을 input으로 사용하여 subpixel의 motion을 고려하여 HR framne을 output한다. 거의 모든 DNN 기반 알고리즘은 두가지 단계로 구성된다: motion estimation, compensation (+up-sampling). 본 프로세스의 문제점은 결과가 motion estimation의 정확도에 지나치게 의존한다는 것이다(motion estimation에서 occlusion이나 blur가 있는 frame이 있다면 성능이 굉장히 떨어지게 되는 것을 의미함). 또 발생할 수 있는 다른 문제는 CNN을 통해 motion compensated LR frame들의 mixing vlaue로부터 HR output frame이 생성되는데 이로 인해 blurry된 frame을 얻을 수 있다는 것이다. 따라서, 본 논문에서는 explicit motion compensation 방법을 사용하지 않고, motion information을 implicit하게 활용하여 dynamic upsampling filter (DUF)를 생성한다. 이를 통해 local filtering으로 HR frame을 바로 생성하는 방법을 사용하였다. Method- Dynamic Upsampling Filter 기존 upsampling filter의 값은 고정된 값이기 때문에 부자연스러운 결과가 생성된다 (bilinear, bicubic, ..). 저자들은 LR frames의 각 픽셀의 spatio-temporal한 neighborhood에 기반한 upsampling filter인 DUF를 제안하였다. 위 그림은 DUF를 통해 upsampling하는 것을 보여준다. Input frames로 dynamic filter generation network으로 학습하여 $r^2HW$의 set을 가지는 upsampling filter $F_t$를 생성한다. 이때, filter의 size는 특정하기 나름이다 (예시는 5x5). 첨부한 그림은 x4 upsampling의 예시인데, input frames 중 center frame인 $X_t$의 (3,3)을 upsampling한다고 가정하자. 이때는 x4 upsampling이기 때문에 1개의 픽셀 당 16개의 upsampling filter가 사용된다. HR frame ($\\bar{Y}_t$)에 상응하는 좌표인 (3,3) -&amp;gt; (12,12)에서 16개의 새로운 픽셀이 생성되는 것을 확인할 수 있다. 위 과정을 수식을 통해 정리한다면 다음과 같다.$ \\bar{Y}_t(yr+v, xr+u)=\\sum_{j=-2}^{2}\\sum_{i=-2}^{2}F^{x,y,v,u}_t(j+2, i+2)X_t(y+j, x+i) $이때, x,y는 LR frame의 좌표이고 u,v는 $r$ x $r$ block의 좌표이다( 0 $\\le$ $v,u$ $\\le$ $r$-1 ). 예를 들어, r=4인 경우에는 0부터 3이 v,u의 값이 된다. DUF가 기존과 다른 점은 기존의 DL 기반 모델은 feature space에서 여러개의 convolution을 통해 HR frame을 reconstruct하는 방법을 학습하지만, 본 논문에서는 가장 좋은 upsampling filters를 학습하기 위해 DL을 사용한다.- Residual Learning 기존의 방법은 LR frame을 bicubically upsampled된 image에 residual을 더해주어 output HR frame을 생성하였다. 본 논문에서는 여러 개의 input frames로부터 residual image를 생성하고, DUF를 적용하여 upsampled된 frame에 residual을 더하여 좀 더 spatial shaprness와 temporal consistency가 보장된 HR frame을 생성한다. Network Design 본 그림은 네트워크의 전체 구조이다. 결과적으로 residual image와 DUF 2개를 생성하기 때문에 network의 overhead를 줄이기 위해 weight sharing을 적용하였다. Network는 Dense block을 참고하여 구성하였는데, 내부의 2D convolutional layer를 3D convolutional layer로 변경하여 spatio-temporal features를 학습하도록 하였다. 이는 spatio-temporal feature extraction을 수행하기 위함이다. 본 네트워크를 살펴보면 BN (Batch-Normalization)이 적용되는데, SR task에서는 BN이 오히려 feature의 특성을 없애는 경향이 있어서 사용하지 않는다고 알고 있었는데 적용한 것이 독특했다 (개인적인 생각. 반박 대환영입니다). 앞서 설명한 것과 같이, 1) input frames로 부터 DUF를 생성하기 위해 학습하고2) 생성된 DUF로 center frame에 SR을 수행한 후 3) residual이 더해져서 최종 output frame이 생성되는 것을 확인할 수 있다. Temporal Augmentation 본 그림은 새롭게 적용한 data augmentation의 예시이다. 기존의 방법은 rotation이나 flip을 통해 data를 생성하였다. 저자들은 sampling interval을 도입하여 temporal augmentation을 적용하였다. 예를 들어, TA가 2인 경우는 t, t+2, ..와 같이 frame의 간격을 2로 두고 forward direction으로 sampling을 하여 motion의 측면에서 더 빠른 것 처럼 data를 사용할 수 있다는 것이다. - 값을 주면 backward direction으로 sampling을 수행하여 좀 더 motion에 있어서 다양한 data를 만들 수 있는 방법이다. ImplementationDatasets 351개의 video dataset에서 160,000개의 144 x 144 GT training data 구성 Derf’s collection에서 coastguard, foreman, gatden, husky valset으로 사용 Training GT data에 Gaussian filter로 smoothing 수행 후, sub-sampling 수행 Input patch는 32x32, 16 batch size Huber loss 사용 Test phase에서 temporal axis로 zero padding 수행 (프레임 수 유지를 위해)Experimental Result 표는 PSNR과 SSIM을 정리한 것이다. 여기서 16L, 28L, 52L은 newwork의 layer 수를 의미한다. 역시 기존의 통념과 같이, layer가 깊어질수록 성능이 올라가는 것을 확인할 수 있다. 특히, 선이 촘촘하게 구성된 frame에 대해서 더 좋은 성능을 보이기도 하였다. DUF가 다른 제안된 예시보다 복잡한 문양이나 detail한 부분들을 더 잘 복원하는 것을 보여준다. 특히, 제안된 기법에서 layer가 깊은 경우, 처마와 같이 촘촘한 부분에서 강한 모습을 보여주었다.Conclusion 본 논문에서는 dynamic upsampling filter와 residual image를 생성하여 VSR 문제를 해결하였다. 특히, explicit motion estimation을 적용하지 않고 motion을 handle하는 방법을 사용하였다. ReferenceDeep Video Super-Resolution Networking Using Dynamic Upsampling Filters Without Explicit Motion Compensation" }, { "title": "리뷰할 논문 리스트", "url": "/posts/paper-list/", "categories": "Paper", "tags": "Paper", "date": "2021-10-10 00:00:00 +0800", "snippet": "개요현재 연구하고 있는 분야가 super-resolution (SR)이다. 특히, 이미지 SR이 아닌, video를 SR하는 Video SR (이하 VSR)과 관련된 연구를 진행하고 있다. 따라서, 서로 다른 몇 편의 VSR 논문을 리뷰할 예정이다. 리뷰할 논문의 리스트는 다음과 같다.리뷰할 논문 리스트 VSRnet - Video Super-Resolution With Convolutional Neural Networks; IEEE-TCI 2016 DUF - Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion COmpensation; CVPR 2018 EDVR: Video Restoration with Enhanced Deformable COnvolutional Networks BasicVSR: The Search for Essential COmponents in Video Super-Resolution and Beyond; CVPR 2021여러가지 일정들이 겹쳐있어 빠르게 정리할 수 있을지 의문이지만 부지런히 해보자." } ]
